{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WL CVD graphene measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect all .ibw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "owncloud_data = '/Users/nik/owncloud/'\n",
    "experiment_data = 'dil_fridge/Ali/CVD, Nov. 2015_II'\n",
    "\n",
    "local_data = '/Users/nik/Data/CVD_graphene_nov2015'\n",
    "\n",
    "# source_data = os.path.join(owncloud_data, experiment_data)\n",
    "source_data = os.path.join(owncloud_data, experiment_data)\n",
    "os.chdir(local_data)\n",
    "\n",
    "def find_ibw(directory):\n",
    "    \"\"\" find *.ibw files in directory and subdirectories \"\"\"\n",
    "    \n",
    "    ibw = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.ibw'):\n",
    "                ibw.append(os.path.join(root, file))\n",
    "    return ibw\n",
    "\n",
    "def update_data_files(source_data, local_data):\n",
    "    \"\"\" look in source data folder for any new *.ibw files and copy them to local_data \"\"\"\n",
    "    \n",
    "    src = find_ibw(source_data)\n",
    "    local = find_ibw(local_data)\n",
    "    local = [l.split('/')[-1] for l in local]\n",
    "    \n",
    "    moved = []\n",
    "    for s in src:\n",
    "        datname = s.split('/')[-1]\n",
    "        if datname in local:\n",
    "            continue\n",
    "        else:\n",
    "            shutil.copy(os.path.join(source_data, s), os.path.join(local_data, datname))\n",
    "            moved.append(s)\n",
    "    return moved\n",
    "\n",
    "print update_data_files(source_data, local_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use('/Users/nik/Dropbox/Notebooks/folklabrc.py')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%aimport igor\n",
    "from igor.binarywave import Waves\n",
    "import lmfit\n",
    "from lmfit import Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt at SRS Calibration (may be useful later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def guess_range(volts):\n",
    "    ranges = np.array([0.0, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0])\n",
    "    ranges = zip(ranges[0:-1], ranges[1:])\n",
    "    for low, high in ranges:\n",
    "        if (volts*1000.0>=low) & (volts*1000.0<=high):\n",
    "            return high\n",
    "\n",
    "srs6_calibration = {0.5:0.992256, 1.0:0.992920, 2.0:0.993104, 5.0:0.993599, 10.0:0.993866, 20.0:0.993396, 50.0:1.0}\n",
    "srs5_calibration = {0.5:0.993360, 1.0:0.992577, 2.0:0.992749, 5.0:0.993284, 10.0:0.993404, 20.0:0.993106, 50.0:1.0}\n",
    "srs8_calibration = {0.5:0.999229, 1.0:0.998049, 2.0:0.999247, 5.0:1.000250, 10.0:1.000775, 20:1.000286, 50.0:1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Load datasets into dataframes and save metadata as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below are a few useful functions for loading data sets\n",
    "\n",
    "def create_metadata_frame(datnum, sort_by = None, **kwargs):\n",
    "    columns = ['datnum', 'sweeps', 'sweep_type', 'Tmix', 'V_gate', 'B_parallel', 'I_bias']\n",
    "    df = pd.DataFrame(np.zeros((len(datnum),len(columns))), columns = columns)\n",
    "    df['datnum'] = datnum\n",
    "    for k in kwargs:\n",
    "        df[k] = kwargs[k]\n",
    "    df.sort(columns='datnum', axis=0, ascending=True, inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    for ind in df.index:\n",
    "        if ind not in df.index:\n",
    "            continue\n",
    "        if df.loc[ind, 'sweep_type']=='multiple':\n",
    "            n = df.loc[ind,'sweeps']\n",
    "            dat = df.loc[ind, 'datnum']\n",
    "            drop = [d for d in range(dat+1, dat+sweeps)]\n",
    "            df = df[~df['datnum'].isin(drop)]\n",
    "    if sort_by:\n",
    "        df.sort(columns=sort_by, axis=0, ascending=True, inplace=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_Tmix(df):\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            w = Waves('dat{0}mixchtemp.ibw'.format(row['datnum']))\n",
    "            row['Tmix'] = w.y.mean()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "# I think you want to add a Tset column\n",
    "# round to the nearest 10mK below 1000mK\n",
    "# round to the nearest 200mK above 1000mK\n",
    "            \n",
    "def add_wlfit_columns(df):\n",
    "    numcols = df['sweeps'].max()\n",
    "    parameters = ['bo', 'ro', 'Bphi', 'Bi', 'Bstar', 'lin', 'A']         \n",
    "    for p in parameters: \n",
    "        df[p] = 0.0\n",
    "        df[p+'_stderr'] = 0.0\n",
    "    for i in range(numcols):\n",
    "        parametersn = [p+str(i) for p in parameters]\n",
    "        for p in parametersn: \n",
    "            df[p] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here are a bunch of code blocks, each representing a new data set\n",
    "\n",
    "# ### T = 6K, V_gate = 0V, multiple B_parallel \n",
    "# # Silvia: I do not think we did change the bpar here, at least I did not analyze this data.\n",
    "\n",
    "# ### 1776mK, 100nA bias, sweep B parallel ###\n",
    "\n",
    "# datnums = [120, 126, 132, 138, 144, 163, 169, 175, 181, 187, \n",
    "#            193, 199, 211, 227, 233, 239, 245, 251, 257, 263, \n",
    "#            269, 275, 281, 287, 293, 299]\n",
    "# fields = [-0.5, -0.25, 0.0, 0.25, 0.5, 1.5, 1, 0.5, 0.3, 0.2, 0.1, \n",
    "#           0.0, 0.0, 3.0, 2.5, 2.0, 0.1, 2.75, 2.25, 1.75, 1.25, \n",
    "#           0.75, 0.1, 0.0, -0.1, -0.2]\n",
    "# d = {'B_parallel':fields, 'I_bias':100.0, 'V_gate':0.0, 'Tmix':100.0, \n",
    "#       'sweep_type':'2d', 'sweeps':6}\n",
    "# df = create_metadata_frame(datnums, **d)\n",
    "\n",
    "# ### 100mK, 4nA bias, sweep B parallel ###\n",
    "\n",
    "# datnums = [n for n in range(1439,1465+1)] # all 2d sweeps\n",
    "# fields = [1.0, 0.75, 0.6, 0.5, 0.4, 0.3, 0.2, 0.18, 0.15, 0.12, 0.09, 0.06, \n",
    "#           0.05, 0.04, 0.03, 0.02, 0.015, 0.01, 0.0075, 0.005, 0.0025, 0.0, \n",
    "#           -0.005, -0.01, -0.02, -0.03, -0.04] # T\n",
    "# d = {'B_parallel':fields, 'I_bias':4.0, 'V_gate':40.0, 'Tmix':100.0, \n",
    "#       'sweep_type':'2d', 'sweeps':6}\n",
    "# df = create_metadata_frame(datnums, **d)\n",
    "\n",
    "# ### T = ~16mK, V_gate = 0V, multiple B_parallel ###\n",
    "\n",
    "# # Silvia: there were other measurements before this\n",
    "# #         but I did not carefully analyze them because they had a lot of bias.\n",
    "\n",
    "# # This was measured from Nov26 to Nov28. with 2nA/2f.5nA bias.\n",
    "\n",
    "# bpars=[0.5,0.4,0.3,0.2,0.18,0.15,0.12,0.09,0.06\n",
    "#        ,0.05,0.04,0.03,0.02,0.015,0.01,0.005,0,-0.005,-0.01,-0.02];\n",
    "# filenum=[762:7:902];\n",
    "# bpars=-[0.03,0.04,0.05];\n",
    "# filenum=[n for n in range(902,917+1,5)];\n",
    "\n",
    "# bpars=[0.005,0.005]\n",
    "# filenum=[n for n in 918:921]\n",
    "\n",
    "# # and one more time:\n",
    "\n",
    "# bpars=[0.5,0.4,0.3,0.2,0.18,0.15,0.12,0.09,0.06,0.05,\n",
    "#        0.04,0.03,0.02,0.015,0.01,0.0075,0.005,0.0025,0,-0.005,-0.01,-0.02];\n",
    "# filenum=[n for n in range(943,1053+1, 5)]\n",
    "\n",
    "### V_gate = 40V ###\n",
    "\n",
    "# T = ~16mK, V_gate = 40V, multiple B_parallel\n",
    "\n",
    "# datnums = [1223, 1222, 1221, 1224, 1219, 1229, 1218, 1228, 1217, 1196, \n",
    "#            1226, 1215, 1225, 1214, 1213, 1212, 1211, 1210, 1209, 1208,\n",
    "#            1207, 1206, 1205, 1204, 1203, 1202, 1201, 1200, 1199]\n",
    "# bpars=[1.0,0.5,0.4,0.3,0.2,0.18,0.15,0.12,0.09,0.06,0.05,0.04,\n",
    "#        0.03,0.02,0.015,0.01,0.0075,0.005,0.0025,0,-0.005,-0.01,-0.02,-0.03,-0.04,0.009:-0.003:-0.003]\n",
    "# mdata = {'sweeps':4, 'sweep_type':'2d', 'Tmix':16.0, 'V_gate':40.0, 'B_parallel':bpars, 'I_bias':1.0}\n",
    "# df = create_metadata_frame(datnums, **mdata)\n",
    "# get_Tmix(df)\n",
    "# add_wlfit_columns(df)\n",
    "# df.sort(columns='B_parallel', axis=0, ascending=True, inplace=True)\n",
    "# f = 'additional_data/bpar_16mK_40V_1nA.csv'\n",
    "# if os.path.isfile(f):\n",
    "#     pass\n",
    "# else:\n",
    "#     df.to_csv(f)\n",
    "\n",
    "### V_gate= -40V ###\n",
    "\n",
    "# 18mK, V_gate = -40V, I_bias = 8nA\n",
    "\n",
    "datnums = [n for n in range(1669, 1684+1)]\n",
    "bpars = [0.12, 0.09, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01, 0.005, 0.0, -0.005, -0.01, -0.02, -0.03, -0.04, -0.05]\n",
    "mdata = {'sweeps':6, 'sweep_type':'2d', 'Tmix':18.0, 'V_gate':-40.0, 'B_parallel':bpars, 'I_bias':1.6}\n",
    "df = create_metadata_frame(datnums, sort_by='B_parallel', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/bpar_18mK_n40V_1p6nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "# 100mK, V_gate = -40V, I_bias = 8nA\n",
    "\n",
    "datnums = [n for n in range(1582, 1612+1)]\n",
    "bpars = [0.005, 0.005, 2.0, 1.5, 1.0, 0.75, 0.6, 0.5, 0.4, 0.3, 0.2, 0.18, 0.15, 0.12, 0.09, 0.06, 0.05,\n",
    "         0.04, 0.03, 0.02, 0.015, 0.01, 0.0075, 0.005, 0.0025, 0.0, -0.005, -0.01, -0.02, -0.03, -0.04]\n",
    "mdata = {'sweeps':2, 'sweep_type':'2d', 'Tmix':100.0, 'V_gate':-40.0, 'B_parallel':bpars, 'I_bias':8.0}\n",
    "df = create_metadata_frame(datnums, sort_by='B_parallel', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/bpar_100mK_n40V_8nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "# 200mK, V_gate = -40V, I_bias = 10nA\n",
    "\n",
    "datnums=[n for n in range(1613,1641+1)]\n",
    "bpars=[2.0, 1.5, 1.0, 0.75, 0.6, 0.5, 0.4, 0.3, 0.2, 0.18, 0.15, 0.12, 0.09, 0.06, 0.05, 0.04,\n",
    "       0.03, 0.02, 0.015, 0.01, 0.0075, 0.005, 0.0025, 0.0, -0.005, -0.01, -0.02, -0.03, -0.04]\n",
    "mdata = {'sweeps':2, 'sweep_type':'2d', 'Tmix':200.0, 'V_gate':-40.0, 'B_parallel':bpars, 'I_bias':10.0}\n",
    "df = create_metadata_frame(datnums, **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/bpar_200mK_n40V_10nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "# 400mK, V_gate = -40V, I_bias = 15nA\n",
    "\n",
    "datnums = [n for n in range(1642,1659+1)]\n",
    "bpars =  [0.5, 0.4, 0.3, 0.2, 0.18, 0.15, 0.12, 0.09, 0.06, 0.045, \n",
    "          0.03, 0.015, 0.005, -0.005, -0.02, -0.035, -0.05, -0.08]\n",
    "mdata = {'sweeps':2, 'sweep_type':'2d', 'Tmix':400.0, 'V_gate':-40.0, 'B_parallel':bpars, 'I_bias':15.0}\n",
    "df = create_metadata_frame(datnums, sort_by='B_parallel', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/bpar_400mK_n40V_15nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "# B_parallel = 5mT, V_gate = -40V,\n",
    "\n",
    "datnums = [n for n in range(1660,1668+1)]\n",
    "temps = [302, 220, 180, 150, 120, 90, 70, 50, 30]\n",
    "bias = [12, 10, 10, 8, 8, 5, 4, 2, 2]\n",
    "sweeps = [2, 2, 2, 2, 2, 4, 4, 6, 6]\n",
    "mdata = {'sweeps':sweeps, 'sweep_type':'2d', 'Tmix':temps, 'V_gate':-40.0, 'B_parallel':0.005, 'I_bias':bias}\n",
    "df = create_metadata_frame(datnums, sort_by='Tmix', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/temp_varyI_5mT_n40V.csv'\n",
    "df.to_csv(f)\n",
    "    \n",
    "# B_parallel = 5mT, sweep gate, Tmix = 100mK, I_bias = 8nA\n",
    "\n",
    "datnums = [n for n in range(1685, 1705+1)]\n",
    "gate = [g for g in range(-50, 55, 5)]\n",
    "datnums.extend([n for n in range(1721, 1724+1)])\n",
    "gate.extend([70, 65, 60, 55])\n",
    "datnums.extend([n for n in range(1725, 1728+1)])\n",
    "gate.extend([-55, -60, -65, -70])\n",
    "\n",
    "mdata = {'sweeps':2, 'sweep_type':'2d', 'Tmix':100.0, 'V_gate':gate, 'B_parallel':0.005, 'I_bias':8.0}\n",
    "df = create_metadata_frame(datnums, sort_by='V_gate', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/gate_5mT_100mK_8nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "# B_parallel = 500mT, sweep gate, Tmix = 100mK, I_bias = 8nA\n",
    "\n",
    "datnums = [n for n in range(1729, 1757+1)]\n",
    "gate = [g for g in range(-70, 75, 5)]\n",
    "mdata = {'sweeps':2, 'sweep_type':'2d', 'Tmix':100.0, 'V_gate':gate, 'B_parallel':0.005, 'I_bias':8.0}\n",
    "df = create_metadata_frame(datnums, sort_by='V_gate', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/gate_500mT_100mK_8nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "# Base temperature data as a function of V_gate, I_bias = 1.6nA, B_parallel = 5mT:\n",
    "\n",
    "datnums = [n for n in range(1758, 1772+1)]\n",
    "gate = np.linspace(70, -70, 15)\n",
    "mdata = {'sweeps':6, 'sweep_type':'2d', 'Tmix':18.0, 'V_gate':gate, 'B_parallel':0.005, 'I_bias':1.6}\n",
    "df = create_metadata_frame(datnums, sort_by='V_gate', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/gate_5mT_18mK_1p6nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "# Base temperature data as a function of V_gate, I_bias = 1.6nA, B_parallel = 300mT:\n",
    "\n",
    "datnums = [n for n in range(1773, 1777+1)]\n",
    "gate = np.linspace(-70, 70, 5)\n",
    "mdata = {'sweeps':6, 'sweep_type':'2d', 'Tmix':18.0, 'V_gate':gate, 'B_parallel':0.3, 'I_bias':1.6}\n",
    "df = create_metadata_frame(datnums, sort_by='V_gate', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/gate_300mT_18mK_1p6nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "# gaussian spaced sweeps +/-50mT around WL peaks at 100mK, I_bias = 1.6nA (crap)\n",
    "\n",
    "datnums = [n for n in range(1778, 1787+1)] # one up, one down at each gate\n",
    "gate = [40, 15, -10, -35, -60]\n",
    "sweeps = 2\n",
    "gate = np.repeat(gate, sweeps)\n",
    "mdata = {'sweeps':sweeps, 'sweep_type':'multiple', \n",
    "         'Tmix':100.0, 'V_gate':gate, 'B_parallel':0.005, 'I_bias':1.6}\n",
    "df = create_metadata_frame(datnums, sort_by='V_gate', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/gate_5mT_100mK_1p6nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "datnums = [n for n in range(1788, 1797+1)]\n",
    "gate = [-60, -35, -10, 15, 40]\n",
    "sweeps = 2\n",
    "gate = np.repeat(gate, sweeps)\n",
    "mdata = {'sweeps':sweeps, 'sweep_type':'multiple', 'Tmix':100.0, 'V_gate':gate, 'B_parallel':0.6, 'I_bias':1.6}\n",
    "df = create_metadata_frame(datnums, sort_by='V_gate', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/gate_600mT_100mK_1p6nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "### add these to some other dataset above ###\n",
    "# # sweep over WL peak at 1T, 100mK, 8nA (continuing the plot on page 93):\n",
    "# dat1798-dat1804, gate = [-75, 50, 25, 0, 25, 50, 75]\n",
    "\n",
    "# # (continuing the plot on page 88)\n",
    "# B_parallel = 1500mT, V_gate=-40V, sweep over peak, vary T:\n",
    "# files = dat1805-dat1812, dat1821\n",
    "# temps = [330, 250, 200, 150, 120, 90, 70, 50, 27]\n",
    "# bias = [12, 10, 10, 8, 8, 5, 4, 2, 1.6]\n",
    "# sweeps = [2, 2, 2, 2, 2, 4, 4, 6, 6]\n",
    " \n",
    "# At base temperature, sweep over WL peak as a function of B_parallel, I_bias = 1.6nA \n",
    "#(continuing the plot on page 86):\n",
    "# fields = {1.5, 1.2, 1.0, 0.8, 0.6, 0.4, 0.3, 0.2}\n",
    "# dat1813-dat1820\n",
    "\n",
    "###            ###\n",
    "\n",
    "# 1.5K data \n",
    "\n",
    "datnums = [n for n in range(1827, 1840+1)]\n",
    "gate = [-60, -40, -20, 0, 20, 20, 40]\n",
    "sweeps = 2\n",
    "gate = np.repeat(gate, sweeps)\n",
    "mdata = {'sweeps':sweeps, 'sweep_type':'multiple', 'Tmix':1500.0, 'V_gate':gate, 'B_parallel':0.005, 'I_bias':50.0}\n",
    "df = create_metadata_frame(datnums, sort_by='V_gate', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/gate_5mT_1500mK_50nA.csv'\n",
    "df.to_csv(f)\n",
    "\n",
    "datnums = [n for n in range(1841, 1852+1)]\n",
    "gate = [40, 20, 0, -20, -40, -60]\n",
    "sweeps = 2\n",
    "gate = np.repeat(gate, sweeps)\n",
    "mdata = {'sweeps':sweeps, 'sweep_type':'multiple', 'Tmix':1500.0, 'V_gate':gate, 'B_parallel':1.2, 'I_bias':50.0}\n",
    "df = create_metadata_frame(datnums, sort_by='V_gate', **mdata)\n",
    "get_Tmix(df)\n",
    "add_wlfit_columns(df)\n",
    "f = 'analysis/gate_1200mT_1500mK_50nA.csv'\n",
    "df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### fitting to WL peaks ###\n",
    "\n",
    "e=1.60217646e-19;  mstar=8.724125937209915e-32; h=6.626068e-34; G_0=7.45e-5; hbar=h/2.0/np.pi;\n",
    "# D=5.106250385935459e-03 # diffusion constant\n",
    "\n",
    "from scipy.special import psi\n",
    "\n",
    "# basic fit function\n",
    "def wlcorr(b, bo, ro, Bphi, Bi, Bstar, lin, A):\n",
    "    \"\"\" Weak localization peak fitting function. Adapted from Igor code. \"\"\"\n",
    "    b=b-bo\n",
    "\n",
    "    z1=np.array(abs(b)/Bphi, dtype=np.float64)\n",
    "    F1=np.log(z1)+psi(0.5+1/z1)\n",
    "\n",
    "    z2=np.array(abs(b)/(Bphi+2*Bi), dtype=np.float64)\n",
    "    F2=np.log(z2)+psi(0.5+1/z2)\n",
    "\n",
    "    z3=np.array(abs(b)/(Bphi+Bstar), dtype=np.float64)\n",
    "    F3=np.log(z3)+psi(0.5+1/z3)\n",
    "\n",
    "    return (lin*b) + ro - A*ro**2.0*(e*e/np.pi/h*(F1-F2-2*F3))\n",
    "\n",
    "### parabola to fit curvature only ###\n",
    "def parabola(b, bo, ro, a):\n",
    "    \"\"\" Inverse parabola for fitting  \"\"\"\n",
    "    return ro - a*np.power(b - bo, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def round_to(n, precision):\n",
    "    \"\"\" http://stackoverflow.com/questions/4265546/python-round-to-nearest-05 \"\"\"\n",
    "    correction = 0.5 if n >= 0 else -0.5\n",
    "    return int( n/precision+correction ) * precision\n",
    "\n",
    "def import_2d_dataset(datnum, bias, sample, measured_current = True):\n",
    "    \"\"\" lock in configuration for all of my 2d data is:\n",
    "    \n",
    "        g6: Rxx top\n",
    "        g5: Rxx top\n",
    "        g8: Rxx bottom\n",
    "        g9: Current top \n",
    "        \n",
    "        Returns: rxx dataframe with x and y components \"\"\"\n",
    "    \n",
    "    if sample=='top':\n",
    "        rxx = (Waves('dat{0:d}g6x2d.ibw'.format(int(datnum))).as_dataframe(sharex=True)\n",
    "               +Waves('dat{0:d}g5x2d.ibw'.format(int(datnum))).as_dataframe(sharex=True))/2.0\n",
    "        if measured_current == True:\n",
    "            bias = Waves('dat{0:d}g9x2d.ibw'.format(int(datnum))).y.mean().mean()*1000.0\n",
    "        scale = (1e-3)*(1e9)/(bias)/(1.5)\n",
    "        rxx *= scale\n",
    "    elif sample=='bottom':\n",
    "        rxx = Waves('dat{0:d}g8x2d.ibw'.format(int(datnum))).as_dataframe(sharex=True)\n",
    "        scale = (1e-3)*(1e9)/(bias)/(1.5)\n",
    "        rxx *= scale\n",
    "    rxx.iloc[:, 2::2] = rxx.iloc[:, 2::2].values[::-1, :] # flip downsweeps to correct direction\n",
    "    rxx.x = Waves('dat{0:d}fields.ibw'.format(int(datnum))).y\n",
    "    return rxx\n",
    "\n",
    "# def import_multiple_dataset(datnum, bias, sample, measured_current = True):\n",
    "#     \"\"\" lock in configuration for all of my 2d data is:\n",
    "    \n",
    "#         ???? \n",
    "\n",
    "#         Returns: rxx dataframe with x and y components \"\"\"\n",
    "    \n",
    "\n",
    "def wl_fit(b, rxx, width, height, fix = None, ro=None, bcenter = None):\n",
    "    \"\"\" fit a single b_perpendicular (b) and r_xx set to the WL model.\n",
    "    \n",
    "        Inputs:  width -- range of data over which to fit\n",
    "                 fix -- dictionary of parameters to fix and their values\n",
    "                 ro -- initial guess at ro\n",
    "                 bcenter -- initial guess at bcenter \n",
    "        Outputs: results -- lmfit results object \"\"\"\n",
    "\n",
    "    # create model\n",
    "    model = Model(wlcorr, independent_vars=['b'])\n",
    "    params = model.make_params()\n",
    "    method = 'nelder'\n",
    "\n",
    "    if not bcenter:\n",
    "        bcenter = b.mean()\n",
    "    if not ro:\n",
    "        ro = rxx[find_nearest(b, bcenter)]\n",
    "    \n",
    "    ### data selection criteria ###\n",
    "    right = bcenter+width\n",
    "    left = bcenter-width \n",
    "    bottom = (1-height)*ro\n",
    "\n",
    "    bi = (b > left) & (b < right)\n",
    "    ri = (rxx > bottom)\n",
    "    inds = bi & ri\n",
    "\n",
    "    b = b[inds]\n",
    "    rxx = rxx[inds]\n",
    "    ### end data selection ###\n",
    "\n",
    "    # always vary\n",
    "    params['bo'].value = bcenter\n",
    "    params['ro'].value = round_to(ro, 0.5)\n",
    "    params['Bphi'].value = 0.15\n",
    "\n",
    "    # sometimes vary\n",
    "    params['Bi'].value = 1.8\n",
    "    params['Bstar'].value = 2.5e7\n",
    "    params['lin'].value = 0.0027\n",
    "    params['A'].value = 1.00 \n",
    "        \n",
    "    if fix:\n",
    "        for p in fix:\n",
    "            params[p].vary = False\n",
    "            params[p].value = fix[p]\n",
    "\n",
    "    result = model.fit(rxx, params, b=b, method = method)\n",
    "    return result\n",
    "\n",
    "def get_single_wlfits(df, width, height, fix = None, sample='top'):\n",
    "    \"\"\" fill dataframe with fit results for single sweeps \"\"\"\n",
    "\n",
    "    for ind in df.index:\n",
    "        datnum = df.loc[ind,'datnum'] # get dat file number\n",
    "        sweeps = df.loc[ind,'sweeps']\n",
    "\n",
    "        # import data based on sweep type\n",
    "        # the y data is not scaled at all\n",
    "        if df.loc[ind,'sweep_type']=='2d':\n",
    "            rxx = import_2d_dataset(datnum, bias=df.loc[ind,'I_bias'], sample=sample)\n",
    "        \n",
    "        for n in range(sweeps):\n",
    "            c = 'y{0:d}'.format(n)\n",
    "            result = wl_fit(rxx.x, rxx[c], width, height, fix = fix, ro=None, bcenter = None)\n",
    "            for p in result.params:\n",
    "                df.loc[ind,p+str(n)] = result.values[p]\n",
    "                \n",
    "def get_average_wlfits(df, width, height, fix=None, sample='top'):\n",
    "    \n",
    "    for ind in df.index:\n",
    "        datnum = df.loc[ind,'datnum'] # get dat file number\n",
    "        sweeps = df.loc[ind,'sweeps']\n",
    "        \n",
    "        # import data based on sweep type\n",
    "        # the y data is not scaled at all\n",
    "        if df.loc[ind,'sweep_type']=='2d':\n",
    "            rxx = import_2d_dataset(datnum, bias=df.loc[ind,'I_bias'], sample=sample)\n",
    "                                    \n",
    "        y_cols = ['y{0:d}'.format(n) for n in range(sweeps)]\n",
    "        ro_cols = ['ro{0:d}'.format(n) for n in range(sweeps)]\n",
    "        \n",
    "        ydata = rxx[y_cols].values\n",
    "        ydata = ydata.reshape(-1,1)\n",
    "               \n",
    "        xdata = np.tile(rxx.x.values, (sweeps, 1)).transpose()\n",
    "        xdata = xdata - df.loc[ind,ro_cols].values # subtract centers values\n",
    "        xdata = xdata + df.loc[ind, ro_cols].mean() # a\n",
    "        xdata = xdata.reshape(-1,1)\n",
    "        sort_ind = np.argsort(xdata, axis=None)\n",
    "        \n",
    "        xdata = xdata[sort_ind]\n",
    "        ydata = ydata[sort_ind]\n",
    "        \n",
    "        xdata = pd.rolling_mean(xdata, sweeps, center=True)\n",
    "        ydata = pd.rolling_mean(ydata, sweeps, center=True)\n",
    "        ydata = ydata[~np.isnan(ydata)]\n",
    "        xdata = xdata[~np.isnan(xdata)]\n",
    "        \n",
    "        result = wl_fit(xdata, ydata, width, height, fix = fix, ro=None, bcenter = None)\n",
    "        for p in result.params:\n",
    "            pcols = ['{0}{1:d}'.format(p,n) for n in range(sweeps)]\n",
    "            df.loc[ind,p] = result.values[p]\n",
    "            df.loc[ind,p+'_stderr'] = df.loc[ind,pcols].values.std()\n",
    "            \n",
    "### newer stuff, works better ###\n",
    "# here is a great example someone wrote on stackoverflow: \n",
    "# http://stackoverflow.com/questions/20339234/python-and-lmfit-how-to-fit-multiple-datasets-with-shared-parameters\n",
    "\n",
    "# return fit for dataset i\n",
    "def wlcorr_dataset(params, i, b):\n",
    "    \"\"\" Weak localization peak fitting function. Adapted from Igor code. \"\"\"\n",
    "    bo = params['bo{0:d}'.format(i)]\n",
    "    ro = params['ro{0:d}'.format(i)]\n",
    "    Bphi = params['Bphi{0:d}'.format(i)]\n",
    "    Bi = params['Bi{0:d}'.format(i)]\n",
    "    Bstar = params['Bstar{0:d}'.format(i)]\n",
    "    lin = params['lin{0:d}'.format(i)]\n",
    "    A = params['A{0:d}'.format(i)]\n",
    "    return wlcorr(b, bo, ro, Bphi, Bi, Bstar, lin, A)\n",
    "\n",
    "def objective(params, b, data):\n",
    "    \"\"\" calculate total residual for fits to several data sets held\n",
    "    in a 2-D array, and modeled by Gaussian functions\"\"\"\n",
    "    nb, ndata = data.shape\n",
    "    resid = np.zeros(data.shape)\n",
    "    # make residual per data set\n",
    "    for i in range(ndata):\n",
    "        resid[:, i] = data[:, i] - wlcorr_dataset(params, i, b)\n",
    "    # now flatten this to a 1D array, as minimize() needs\n",
    "    return resid.flatten()\n",
    "\n",
    "def wl_fit_simultaneous(x, y, fix = None):\n",
    "\n",
    "    # create parameters, one per data set\n",
    "    fit_params = Parameters()\n",
    "    bcenter = x.mean()\n",
    "    rpeak = y[find_nearest(b, bcenter)].mean()\n",
    "    guess = {'bo':bcenter, 'ro':rpeak, 'Bphi':0.15, 'Bi': 1.75, 'Bstar': 2500000.0, 'lin': 0.0027, 'A': 1.08}\n",
    "    for i in range(y.shape[1]):\n",
    "        fit_params.add('bo{0:d}'.format(i), value = guess['bo'])\n",
    "        fit_params.add('ro{0:d}'.format(i), value = guess['ro'])\n",
    "        fit_params.add('Bphi{0:d}'.format(i), value = guess['Bphi'])\n",
    "        fit_params.add('Bi{0:d}'.format(i), value = guess['Bi'])\n",
    "        fit_params.add('Bstar{0:d}'.format(i), value = guess['Bstar'])\n",
    "        fit_params.add('lin{0:d}'.format(i), value = guess['lin'])\n",
    "        fit_params.add('A{0:d}'.format(i), value = guess['A'])\n",
    "\n",
    "    # fix any parameters\n",
    "    if fix:\n",
    "        for p in fix:\n",
    "            for i in range(y.shape[1]):\n",
    "                fit_params[p+str(i)].vary = False\n",
    "                fit_params[p+str(i)].value = fix[p]\n",
    "\n",
    "    # add constraints for all variables but bo and ro\n",
    "    for i in range(y.shape[1])[1:]:\n",
    "        fit_params['Bphi{0:d}'.format(i)].expr = 'Bphi0'\n",
    "        fit_params['Bi{0:d}'.format(i)].expr = 'Bi0'\n",
    "        fit_params['Bstar{0:d}'.format(i)].expr = 'Bstar0'\n",
    "        fit_params['lin{0:d}'.format(i)].expr = 'lin0'\n",
    "        fit_params['A{0:d}'.format(i)].expr = 'A0'\n",
    "\n",
    "    # # run the global fit to all the data sets\n",
    "    m = minimize(objective, fit_params, args=(x, y))\n",
    "    return fit_params\n",
    "\n",
    "def simultaneous_wlfits(df, fix = None, sample='top'):\n",
    "    \"\"\" fill dataframe with fit results for single sweeps \"\"\"\n",
    "\n",
    "    for ind in df.index:\n",
    "        datnum = df.loc[ind,'datnum'] # get dat file number\n",
    "        sweeps = df.loc[ind,'sweeps']\n",
    "\n",
    "        # import data based on sweep type\n",
    "        # the y data is not scaled at all\n",
    "        if df.loc[ind,'sweep_type']=='2d':\n",
    "            rxx = import_2d_dataset(datnum, bias=df.loc[ind,'I_bias'], sample=sample)\n",
    "        \n",
    "            params = wl_fit_simultaneous(rxx.x.values, rxx.iloc[:,1:].values, fix = fix)\n",
    "            for p in params:\n",
    "                df.loc[ind,p] = params[p].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_multiple_dataset(datnum, sweeps, bias, sample, measured_current = True):\n",
    "    \"\"\" lock in configuration for all of my 2d data is:\n",
    "    \n",
    "        ???? \n",
    "\n",
    "        Returns: rxx dataframe with x and y components \"\"\"\n",
    "    dats = [d for d in range(datnum, datnum+sweeps)]\n",
    "    \n",
    "    \n",
    "    if sample=='top':\n",
    "        rxx = (Waves('dat{0:d}g6x.ibw'.format(int(datnum))).as_dataframe(sharex=True)\n",
    "               +Waves('dat{0:d}g5x.ibw'.format(int(datnum))).as_dataframe(sharex=True))/2.0\n",
    "        if measured_current == True:\n",
    "            bias = Waves('dat{0:d}g9x.ibw'.format(int(datnum))).y.mean().mean()*1000.0\n",
    "        scale = (1e-3)*(1e9)/(bias)/(1.5)\n",
    "        rxx *= scale\n",
    "    elif sample=='bottom':\n",
    "        rxx = Waves('dat{0:d}g8x.ibw'.format(int(datnum))).as_dataframe(sharex=True)\n",
    "        scale = (1e-3)*(1e9)/(bias)/(1.5)\n",
    "        rxx *= scale\n",
    "        \n",
    "        \n",
    "    rxx.iloc[:, 2::2] = rxx.iloc[:, 2::2].values[::-1, :] # flip downsweeps to correct direction\n",
    "    rxx.x = Waves('dat{0:d}fields.ibw'.format(int(datnum))).y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
